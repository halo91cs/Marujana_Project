{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Important modules ###\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import matplotlib.pylab as plt #No module named matplotlib.pylab.. sudo apt-get install python-matplotlib\n",
    "import matplotlib.pyplot as plt2\n",
    "import matplotlib.image as mpimg\n",
    "import operator\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras\n",
    "import tensorflow_hub as hub # No module named tensorflow_hub -- pip install tensorflow-hub\n",
    "\n",
    "from keras.applications import resnet50\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We classify users as personal or non personal users based on their marujana conversations\n",
    "### We used a pretrained image classification model called Resnet to retrieve recognized elements from the images\n",
    "\n",
    "### Resnet is a deep learning image classification model, built with imagenet dataset.\n",
    "### It produces keywords of elements that it recognizes in the email along with probabilities.\n",
    "### I have taken the top 20 keywords for each profile picture.\n",
    "\n",
    "#Load the ResNet50 model\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    "path_marujana_imgs=\"~/marujana_imgs\"\n",
    "\n",
    "imgInfo = {}\n",
    "\n",
    "for filename in os.listdir(path_marujana_imgs):\n",
    "    if filename.endswith(\".jpg\"):       \n",
    "        userType = filename.split('.')[1]\n",
    "        userName = filename.split('.')[0]\n",
    "        try:\n",
    "            original = load_img(filename, target_size=(224, 224))\n",
    "            numpy_image = img_to_array(original)\n",
    "            image_batch = np.expand_dims(numpy_image, axis=0)\n",
    "            processed_image = resnet50.preprocess_input(image_batch.copy())\n",
    "\n",
    "            predictions = resnet_model.predict(processed_image)\n",
    "            label = decode_predictions(predictions, top=20) ## We get predictions for each profile image\n",
    "            print(label)\n",
    "            imgInfo[len(imgInfo)] = {'User Type': userType, 'Username': userName, 'Label': label}\n",
    "            \n",
    "        except:\n",
    "            print(\"Error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We used Googleâ€™s pre-trained Word2Vec model to obtain the numerical embedding vector of the keywords that represent the profile pictures.\n",
    "\n",
    "# Explore Google's huge Word2Vec model.\n",
    "\n",
    "import gensim\n",
    "import logging\n",
    "\n",
    "# Logging code taken from http://rare-technologies.com/word2vec-tutorial/\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "#model = gensim.models.Word2Vec.load_word2vec_format('~/GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('~/path/to/GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average each probability.\n",
    "avgProbabilities = []\n",
    "imgInfoDf = pd.DataFrame(imgInfo).T\n",
    "\n",
    "### We averaged 20 probabilities for each image\n",
    "for i in imgInfoDf['Label']:\n",
    "    for j in range(0, 20):\n",
    "        word = i[j][1]\n",
    "        modelOutput += model(word)\n",
    "    \n",
    "    modelOutput = modelOutput / 20\n",
    "    avgProbabilities.append(modelOutput)\n",
    "\n",
    "imgInfoDf['Average Probabilities'] = avgProbabilities    \n",
    "\n",
    "## np_users: non-personal\n",
    "## p_users: personal\n",
    "np_users = imgInfoDf[(imgInfoDf['User Type'] == 'I') | (imgInfoDf['User Type'] == 'I1') | \n",
    "                     (imgInfoDf['User Type'] == 'R') | (imgInfoDf['User Type'] == 'R1') |\n",
    "                     (imgInfoDf['User Type'] == 'NA') | (imgInfoDf['User Type'] == 'NA1')]['Average Probabilities']\n",
    "\n",
    "p_users = imgInfoDf[(imgInfoDf['User Type'] == 'P') | (imgInfoDf['User Type'] == 'P1')]\n",
    "\n",
    "X = imgInfoDf['Average Probabilities']\n",
    "y = []\n",
    "\n",
    "for i in range(len(np_users)):\n",
    "    y.append(0)\n",
    "\n",
    "for j in range(len(p_users)):\n",
    "    y.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model on the training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "# make class predictions for the testing set\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(metrics.classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
